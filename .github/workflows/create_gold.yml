name: Create GCS Bucket & BQ Table

on:
    push:
        paths:
            - 'gold_layer/gold_main.py' 

jobs:
  create-resources:
    runs-on: ubuntu-latest
    env:
      SCHEMA_FILE: gold_layer/ridership_schema.json
      BUCKET_NAME: dataproc-cluster-spark-jars
      TABLE_NAME: nyc
      INCREMENTAL_TABLE_NAME: inc_nyc

    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Create JSON Credentials File
        uses: jsdaniell/create-json@v1.2.3
        with:
          name: "gcloud-service-key.json"
          json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          export_default_credentials: true

      - name: Authenticate to Google Cloud
        run: gcloud auth activate-service-account --key-file=gcloud-service-key.json
      
      - name: Generate 4-character hash for bucket name
        id: hash
        run: |
          HASH=$(echo "${{ secrets.GCP_PROJECT_ID }}$(date +%s)" | sha1sum | cut -c1-4)
          UNIQUE_BUCKET_NAME="rakshaka-dataproc-bucket-$HASH"
          echo "bucket=$UNIQUE_BUCKET_NAME" >> $GITHUB_OUTPUT

      - name: Create GCS Bucket
        run: |
          gsutil mb -p ${{ secrets.GCP_PROJECT_ID }} -l US gs://${{ steps.hash.outputs.bucket }}

      - name: Upload files to GCS
        run: |
          echo "Upload gold layer files to GCS bucket ${BUCKET_NAME}"
          gsutil -m rsync -r gold_layer/ gs://${BUCKET_NAME}

      - name: Create BigQuery Dataset if not exists
        run: |
          if ! bq ls --project_id=${{ secrets.GCP_PROJECT_ID }} | grep -q "${{ secrets.BQ_DATASET }}"; then
            bq --location=US mk --dataset \
              --project_id=${{ secrets.GCP_PROJECT_ID }} \
              ${{ secrets.BQ_DATASET }}
          else
            echo "Dataset ${{ secrets.BQ_DATASET }} already exists"
          fi

      - name: Create BigQuery Table if not exists
        run: |
          if ! bq show --format=prettyjson ${{ secrets.GCP_PROJECT_ID }}:${{ secrets.BQ_DATASET }}.${{ env.TABLE_NAME }} > /dev/null 2>&1; then
            bq mk --table \
              --project_id=${{ secrets.GCP_PROJECT_ID }} \
              ${{ secrets.BQ_DATASET }}.${{ env.TABLE_NAME }} \
              ${{ env.SCHEMA_FILE }}
          else
            echo "Table ${{ secrets.BQ_DATASET }}.${{ env.TABLE_NAME }} already exists"
          fi

    
      - name: Create BigQuery Table for incremental load
        run: |
          bq mk --table \
          --project_id=${{ secrets.GCP_PROJECT_ID }} \
          ${{ secrets.BQ_DATASET }}.${{ env.INCREMENTAL_TABLE_NAME }} \
          ${{ env.SCHEMA_FILE }}
     
      - name: Submit PySpark Job to Dataproc
        run: |
          gcloud dataproc jobs submit pyspark \
            --cluster=rakshaka \
            --region=us-central1 \
            gs://dataproc-cluster-spark-jars/gold_main.py \
            --files=gs://dataproc-cluster-spark-jars/gold_main.py \
            -- \
            --delta_table_path="gs://nyc_transit_delta_lake/" \
            --temp_gcs_bucket_main="nyc_transit_temp_bq_nyc/" \
            --temp_gcs_bucket_incr="nyc_transit_temp_bq_inc_nyc/" \
            --project_id=${{ secrets.GCP_PROJECT_ID }} \
            --dataset_id=${{ secrets.BQ_DATASET }} \
            --bq_table_id="nyc" \
            --inc_bq_table_id="inc_nyc"
