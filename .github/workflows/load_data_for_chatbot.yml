name: Load Data for Chatbot

on:
  push:
    paths:
      - 'gold_layer/bq_data_for_chatbot.py'

jobs:
  load-data-for-chatbot:
    runs-on: ubuntu-latest

    env:
      BUCKET_NAME: dataproc-cluster-spark-jars
      TABLE_NAME: nyc
      CLUSTER_NAME: rakshaka

    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11' # Specify the Python version you need

      - name: Create JSON Credentials File
        uses: jsdaniell/create-json@v1.2.3
        with:
          name: "gcloud-service-key.json"
          json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          export_default_credentials: true

      - name: Authenticate to Google Cloud
        run: |
          echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}" > gcloud-service-key.json
          gcloud auth activate-service-account --key-file=gcloud-service-key.json
          export GOOGLE_APPLICATION_CREDENTIALS="gcloud-service-key.json"

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install google-cloud-bigquery

      - name: Upload files to GCS
        run: |
          echo "Upload gold layer files to GCS bucket ${BUCKET_NAME}"
          gsutil -m rsync -r gold_layer/ gs://${BUCKET_NAME}

      - name: Check if Spark Cluster is Active
        id: check-cluster
        run: |
          if gcloud dataproc clusters describe $CLUSTER_NAME --region=us-central1; then
            echo "Cluster $CLUSTER_NAME is active."
            echo "cluster_active=true" >> $GITHUB_OUTPUT
          else
            echo "Cluster $CLUSTER_NAME is not active."
            echo "cluster_active=false" >> $GITHUB_OUTPUT
          fi

      - name: Check if BigQuery Table Exists
        id: check-table
        run: |
          if bq show --format=prettyjson ${{ secrets.GCP_PROJECT_ID }}:${{ secrets.BQ_DATASET }}.$TABLE_NAME > /dev/null 2>&1; then
            echo "Table ${{ secrets.BQ_DATASET }}.$TABLE_NAME exists."
            echo "table_exists=true" >> $GITHUB_OUTPUT
          else
            echo "Table ${{ secrets.BQ_DATASET }}.$TABLE_NAME does not exist."
            echo "table_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Proceed if Cluster and Table conditions are satisfied
        if: steps.check-cluster.outputs.cluster_active == 'true' && steps.check-table.outputs.table_exists == 'true'
        run: |
          echo "All conditions satisfied. Proceeding with the workflow."

      - name: Inform if conditions are not satisfied
        if: steps.check-cluster.outputs.cluster_active != 'true' || steps.check-table.outputs.table_exists != 'true'
        run: |
          echo "Conditions not satisfied. Please ensure the Spark cluster is active and the BigQuery table exists."
          exit 1

      - name: Load data for chatbot
        if: steps.check-cluster.outputs.cluster_active == 'true' && steps.check-table.outputs.table_exists == 'true'
        env:
          GOOGLE_APPLICATION_CREDENTIALS: "gcloud-service-key.json"
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET: ${{ secrets.BQ_DATASET }}
        run: |
          gsutil cp gs://${BUCKET_NAME}/bq_data_for_chatbot.py .
          python bq_data_for_chatbot.py

